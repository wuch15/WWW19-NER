{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import *\n",
    "file='training'\n",
    "with open(file,'r')as f:\n",
    "    gold=f.readlines()\n",
    "texts=[]\n",
    "label_gold=[]\n",
    "sentence=[]\n",
    "char_ner_label=[]\n",
    "\n",
    "label_dic={'B-LOC': [1,0,0,0,0,0,0], 'I-LOC': [0,1,0,0,0,0,0],\n",
    " 'B-ORG': [0,0,1,0,0,0,0], 'I-ORG': [0,0,0,1,0,0,0],\n",
    " 'B-PER': [0,0,0,0,1,0,0], 'I-PER': [0,0,0,0,0,1,0],\n",
    " 'N': [0,0,0,0,0,0,1]}\n",
    "symbol=['。','，',',',';','》','?','!','？','！']\n",
    "maxlen=0\n",
    "for i in gold:\n",
    "    if len(i.split())==2:\n",
    "\n",
    "        if len(sentence)>90 and i.split()[1]=='N':\n",
    "            texts.append(sentence)\n",
    "            label_gold.append(char_ner_label)\n",
    "            if len(sentence)>maxlen:\n",
    "                maxlen=len(sentence)\n",
    "            print(len(sentence))\n",
    "            sentence = []\n",
    "            char_ner_label = []\n",
    "            \n",
    "        if len(sentence)>90 and (i.split()[1]=='B-LOC' or i.split()[1]=='B-ORG' or i.split()[1]=='B-PER' ):\n",
    "            texts.append(sentence)\n",
    "            label_gold.append(char_ner_label)\n",
    "            if len(sentence)>maxlen:\n",
    "                maxlen=len(sentence)\n",
    "            print(len(sentence))\n",
    "            sentence = []\n",
    "            char_ner_label = []\n",
    "        sentence.append(i.split()[0])\n",
    "        char_ner_label.append(label_dic[i.split()[1]])\n",
    "    else:\n",
    "        texts.append(sentence)\n",
    "        label_gold.append(char_ner_label)\n",
    "        if len(sentence)>maxlen:\n",
    "            maxlen=len(sentence)\n",
    "        print(len(sentence))\n",
    "        sentence = []\n",
    "        char_ner_label = []\n",
    "\n",
    "\n",
    "\n",
    "from pyltp import Segmentor\n",
    "segmentor = Segmentor()\n",
    "segmentor.load('cws.model')\n",
    "\n",
    "seg_labels = []\n",
    "for i in range(len(texts)):\n",
    "    words = segmentor.segment(''.join(texts[i]))\n",
    "    seg_labels_temp = []\n",
    "\n",
    "    for word in words:\n",
    "\n",
    "        if len(unicode(word))==1:\n",
    "            seg_labels_temp += [[1,0,0,0]]\n",
    "        if len(unicode(word))==2:\n",
    "            seg_labels_temp += [[0,1,0,0]]\n",
    "            seg_labels_temp += [[0,0,0,1]]\n",
    "        if len(unicode(word))>2 :\n",
    "            seg_labels_temp += [[0,1,0,0]]\n",
    "            seg_labels_temp += [[0,0,1,0]]*(len(unicode(word))-2)\n",
    "            seg_labels_temp += [[0,0,0,1]]\n",
    "\n",
    "    if len(seg_labels_temp)!=len(texts[i]):\n",
    "        print(list(words))\n",
    "        print(len(seg_labels_temp),len(texts[i]))\n",
    "        for i in words:\n",
    "            print(i)\n",
    "    assert len(seg_labels_temp)==len(texts[i])\n",
    "    seg_labels.append(seg_labels_temp)\n",
    "\n",
    "\n",
    "file='gold8'\n",
    "with open(file,'r')as f:\n",
    "    test_gold=f.readlines()\n",
    "test_texts=[]\n",
    "test_label_gold=[]\n",
    "sentence=[]\n",
    "char_ner_label=[]\n",
    "\n",
    "maxlen=0\n",
    "\n",
    "for i in test_gold:\n",
    "\n",
    "    if len(i.split())==2:\n",
    "        if len(sentence)>90 and i.split()[1]=='N':\n",
    "            test_texts.append(sentence)\n",
    "            test_label_gold.append(char_ner_label)\n",
    "            if len(sentence)>maxlen:\n",
    "                maxlen=len(sentence)\n",
    "            print(len(sentence))\n",
    "            sentence = []\n",
    "            char_ner_label = []\n",
    "            \n",
    "        if len(sentence)>90 and (i.split()[1]=='B-LOC' or i.split()[1]=='B-ORG' or i.split()[1]=='B-PER' ):\n",
    "            test_texts.append(sentence)\n",
    "            test_label_gold.append(char_ner_label)\n",
    "            if len(sentence)>maxlen:\n",
    "                maxlen=len(sentence)\n",
    "            print(len(sentence))\n",
    "            sentence = []\n",
    "            char_ner_label = []\n",
    "        sentence.append(i.split()[0])\n",
    "        char_ner_label.append(label_dic[i.split()[1]])\n",
    "    else:\n",
    "        test_texts.append(sentence)\n",
    "        test_label_gold.append(char_ner_label)\n",
    "        if len(sentence)>maxlen:\n",
    "            maxlen=len(sentence)\n",
    "        print(len(sentence))\n",
    "        sentence = []\n",
    "        char_ner_label = []\n",
    "        \n",
    "print(maxlen)\n",
    "\n",
    "test_seg_labels = []\n",
    "for i in range(len(test_texts)):\n",
    "    words = segmentor.segment(''.join(test_texts[i]))\n",
    "    seg_labels_temp = []\n",
    "\n",
    "    for word in words:\n",
    "        #print(len(unicode(word)))\n",
    "        if len(unicode(word))==1:\n",
    "            seg_labels_temp += [[1,0,0,0]]\n",
    "        if len(unicode(word))==2:\n",
    "            seg_labels_temp += [[0,1,0,0]]\n",
    "            seg_labels_temp += [[0,0,0,1]]\n",
    "        if len(unicode(word))>2 :\n",
    "            seg_labels_temp += [[0,1,0,0]]\n",
    "            seg_labels_temp += [[0,0,1,0]]*(len(unicode(word))-2)\n",
    "            seg_labels_temp += [[0,0,0,1]]\n",
    "\n",
    "    if len(seg_labels_temp)!=len(test_texts[i]):\n",
    "        print(list(words))\n",
    "        print(len(seg_labels_temp),len(test_texts[i]))\n",
    "        for i in words:\n",
    "            print(i)\n",
    "    assert len(seg_labels_temp)==len(test_texts[i])\n",
    "    test_seg_labels.append(seg_labels_temp)\n",
    "\n",
    "\n",
    "\n",
    "word_dict={'PADDING':0,'UNK':1}\n",
    "\n",
    "for i in gold:\n",
    "    if len(i.split())==2:\n",
    "        sentence.append(i.split()[0])\n",
    "        if not word_dict.has_key(str(i.split()[0])):\n",
    "            word_dict[str(i.split()[0])]=len(word_dict)\n",
    "for i in test_gold:\n",
    "    if len(i.split())==2:\n",
    "        sentence.append(i.split()[0])\n",
    "        if not word_dict.has_key(str(i.split()[0])):\n",
    "            word_dict[str(i.split()[0])]=len(word_dict)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "embdict={}\n",
    "plo=0\n",
    "import cPickle as pickle\n",
    "with open('embedding.txt','rb')as f:\n",
    "    header = f.readline()\n",
    "    vocab_size, layer1_size = map(int, header.split())\n",
    "    binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "    #print(binary_len)\n",
    "    print(vocab_size,layer1_size)\n",
    "    for line in range(vocab_size):\n",
    "        linesplit = f.readline().split()\n",
    "        #print(line,len(k))\n",
    "        word=linesplit[0]\n",
    "        if len(word) != 0:\n",
    "            vec=[float(x) for x in linesplit[1:]]\n",
    "            if word_dict.has_key(str(word)):\n",
    "                embdict[str(word)]=vec\n",
    "\n",
    "\n",
    "from numpy.linalg import cholesky\n",
    "\n",
    "charemb=[0]*len(word_dict)\n",
    "xp=np.zeros(200,dtype='float32')\n",
    "\n",
    "char_emb_voca=[]\n",
    "for i in embdict.keys():\n",
    "    charemb[word_dict[i]]=np.array(embdict[i],dtype='float32')\n",
    "    char_emb_voca.append(charemb[word_dict[i]])\n",
    "char_emb_voca=np.array(char_emb_voca,dtype='float32')\n",
    "\n",
    "mu=np.mean(char_emb_voca, axis=0)\n",
    "Sigma=np.cov(char_emb_voca.T)\n",
    "\n",
    "norm=np.random.multivariate_normal(mu, Sigma, 1)\n",
    "print(mu.shape,Sigma.shape,norm.shape)\n",
    "\n",
    "for i in range(len(charemb)):\n",
    "    if type(charemb[i])==int:\n",
    "        charemb[i]=np.reshape(norm, 200)\n",
    "charemb[0]=np.zeros(200,dtype='float32')\n",
    "charemb=np.array(charemb,dtype='float32')\n",
    "print(charemb.shape)\n",
    "\n",
    "numbict={}\n",
    "for key in word_dict.keys():\n",
    "    numbict[word_dict[key]]=str(key)\n",
    "\n",
    "def same_sample(string,target,name=False):\n",
    "    temp=target\n",
    "    random.shuffle(temp)\n",
    "    if not name:\n",
    "        for k in temp:\n",
    "            if k!=string and unicode(k)[-1]==unicode(string)[-1] and len(unicode(string))>1:\n",
    "                return k\n",
    "    if name:\n",
    "        for k in temp:\n",
    "            if k!=string  and len(unicode(string))>1:\n",
    "                return k\n",
    "    return string\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import tensorflow as tf  \n",
    "import keras\n",
    "from keras import activations \n",
    "from keras import initializers \n",
    "from keras import constraints \n",
    "from keras import regularizers\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.models import Sequential,Model\n",
    "from keras.activations import *\n",
    "from keras.initializers import *\n",
    "from keras.constraints import *\n",
    "from keras_contrib.layers import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time in range(1):\n",
    "    training_word=[]\n",
    "    training_label=[]\n",
    "    training_label_seg=[]\n",
    "    test_word=[]\n",
    "    test_label=[]\n",
    "    \n",
    "    \n",
    "    maxlen=110\n",
    "    exceed=0\n",
    "    for i in texts:\n",
    "        char_list=[]\n",
    "        for j in i:\n",
    "            char_list.append(word_dict[str(j)])\n",
    "        char_list+=(maxlen-len(char_list))*[0]\n",
    "        training_word.append(char_list)\n",
    "    \n",
    "    for i in test_texts:\n",
    "        char_list=[]\n",
    "        for j in i:\n",
    "            char_list.append(word_dict[str(j)])\n",
    "        char_list+=(maxlen-len(char_list))*[0]\n",
    "        test_word.append(char_list)\n",
    "        \n",
    "    for i in label_gold:\n",
    "        training_label.append(i+[[0,0,0,0,0,0,1]]*(maxlen-len(i)))\n",
    "        \n",
    "    for i in test_label_gold:\n",
    "        test_label.append(i+[[0,0,0,0,0,0,1]]*(maxlen-len(i)))\n",
    "    \n",
    "    for i in seg_labels:\n",
    "        training_label_seg.append(i+[[0,0,0,1]]*(maxlen-len(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time in range(1):\n",
    "\n",
    "    import random\n",
    "    sample_number=int(len(training_word)*0.05)\n",
    "    random_index=random.sample(range(len(training_word)),sample_number)\n",
    "    #\n",
    "    training_word=  np.array(training_word,dtype='float32')   [random_index]\n",
    "    training_label= np.array(training_label,dtype='float32')  [random_index]\n",
    "    training_label_seg= np.array(training_label_seg,dtype='float32')[random_index]\n",
    "    \n",
    "    test_word=      np.array(test_word,dtype='float32')\n",
    "    test_label=     np.array(test_label,dtype='float32')\n",
    "    \n",
    "    ner_dict={}\n",
    "    loc_dict={}\n",
    "    per_dict={}\n",
    "    org_dict={}\n",
    "    part=len(training_label)\n",
    "    ner_idlabel=np.argmax(training_label[:part],axis=2)\n",
    "    seg_idlabel=np.argmax(training_label_seg[:part],axis=2)\n",
    "    for i in range(len(ner_idlabel)):\n",
    "        loc_ls=[]\n",
    "        org_ls=[]\n",
    "        per_ls=[]\n",
    "        temp_label=training_label[i]\n",
    "        temp_sent=training_word[i]\n",
    "        loc_temp=[]\n",
    "        org_temp=[]\n",
    "        per_temp=[]\n",
    "        per_begin=-1\n",
    "        per_end=-1\n",
    "        loc_begin=-1\n",
    "        loc_end=-1\n",
    "        org_begin=-1\n",
    "        org_end=-1\n",
    "        for j in range(len(ner_idlabel[i])):\n",
    "            if ner_idlabel[i][j]==0:\n",
    "                if len(loc_temp)>0:\n",
    "                    loc_end=j\n",
    "                    ner_dict[str(''.join(loc_temp))]=seg_idlabel[i][loc_begin:loc_end]\n",
    "                    loc_dict[str(''.join(loc_temp))]=len(loc_dict)\n",
    "                    loc_temp=[]\n",
    "                if len(org_temp)>0:\n",
    "                    org_end=j\n",
    "                    ner_dict[str(''.join(org_temp))]=seg_idlabel[i][org_begin:org_end]\n",
    "                    org_dict[str(''.join(org_temp))]=len(org_dict)\n",
    "                    org_temp=[]\n",
    "                    \n",
    "                if len(per_temp)>0:\n",
    "                    per_end=j\n",
    "                    ner_dict[str(''.join(per_temp))]=seg_idlabel[i][per_begin:per_end]\n",
    "                    per_dict[str(''.join(per_temp))]=len(per_dict)\n",
    "                    per_temp=[]\n",
    "                loc_temp.append(numbict[int(training_word[i][j])])\n",
    "                loc_begin=j\n",
    "            if ner_idlabel[i][j]==1  and len(loc_temp)>0:\n",
    "                loc_temp.append(numbict[int(training_word[i][j])])\n",
    "            if ner_idlabel[i][j]==6 and len(loc_temp)>0:\n",
    "                loc_end=j\n",
    "                ner_dict[str(''.join(loc_temp))]=seg_idlabel[i][loc_begin:loc_end]\n",
    "                loc_dict[str(''.join(loc_temp))]=len(loc_dict)\n",
    "                loc_temp=[]\n",
    "                \n",
    "            if ner_idlabel[i][j]==2:\n",
    "                if len(loc_temp)>0:\n",
    "                    loc_end=j\n",
    "                    ner_dict[str(''.join(loc_temp))]=seg_idlabel[i][loc_begin:loc_end]\n",
    "                    loc_dict[str(''.join(loc_temp))]=len(loc_dict)\n",
    "                    loc_temp=[]\n",
    "                if len(org_temp)>0:\n",
    "                    org_end=j\n",
    "                    ner_dict[str(''.join(org_temp))]=seg_idlabel[i][org_begin:org_end]\n",
    "                    org_dict[str(''.join(org_temp))]=len(org_dict)\n",
    "                    org_temp=[]\n",
    "                if len(per_temp)>0:\n",
    "                    per_end=j\n",
    "                    ner_dict[str(''.join(per_temp))]=seg_idlabel[i][per_begin:per_end]\n",
    "                    per_dict[str(''.join(per_temp))]=len(per_dict)\n",
    "                    per_temp=[]\n",
    "                org_temp.append(numbict[int(training_word[i][j])])\n",
    "                org_begin=j\n",
    "            if ner_idlabel[i][j]==3  and len(org_temp)>0:\n",
    "                org_temp.append(numbict[int(training_word[i][j])])\n",
    "            if ner_idlabel[i][j]==6 and len(org_temp)>0:\n",
    "                org_end=j\n",
    "                ner_dict[str(''.join(org_temp))]=seg_idlabel[i][org_begin:org_end]\n",
    "                org_dict[str(''.join(org_temp))]=len(org_dict)\n",
    "                org_temp=[]\n",
    "        \n",
    "                \n",
    "            if ner_idlabel[i][j]==4:\n",
    "                if len(loc_temp)>0:\n",
    "                    loc_end=j\n",
    "                    ner_dict[str(''.join(loc_temp))]=seg_idlabel[i][loc_begin:loc_end]\n",
    "                    loc_dict[str(''.join(loc_temp))]=len(loc_dict)\n",
    "                    loc_temp=[]\n",
    "                if len(org_temp)>0:\n",
    "                    org_end=j\n",
    "                    ner_dict[str(''.join(org_temp))]=seg_idlabel[i][org_begin:org_end]\n",
    "                    org_dict[str(''.join(org_temp))]=len(org_dict)\n",
    "                    org_temp=[]\n",
    "                if len(per_temp)>0:\n",
    "                    per_end=j\n",
    "                    ner_dict[str(''.join(per_temp))]=seg_idlabel[i][per_begin:per_end]\n",
    "                    per_dict[str(''.join(per_temp))]=len(per_dict)\n",
    "                    per_temp=[]\n",
    "                per_temp.append(numbict[int(training_word[i][j])])\n",
    "                per_begin=j\n",
    "            if ner_idlabel[i][j]==5  and len(per_temp)>0:\n",
    "                per_temp.append(numbict[int(training_word[i][j])])\n",
    "            if ner_idlabel[i][j]==6 and len(per_temp)>0:\n",
    "                per_end=j\n",
    "                ner_dict[str(''.join(per_temp))]=seg_idlabel[i][per_begin:per_end]\n",
    "                per_dict[str(''.join(per_temp))]=len(per_dict)\n",
    "                per_temp=[]\n",
    "            #print(per_begin,per_end,loc_begin,loc_end,org_begin,org_end)\n",
    "            #print(loc_temp,org_temp,per_temp)\n",
    "    loc_dict=loc_dict.keys()\n",
    "    per_dict=per_dict.keys()\n",
    "    org_dict=org_dict.keys()   \n",
    "\n",
    "    #print((per_dict))\n",
    "    \n",
    "    \n",
    "    import random\n",
    "    \n",
    "    ner_idlabel=np.argmax(training_label,axis=2)\n",
    "    \n",
    "    new_train_word=[]\n",
    "    new_train_label=[]\n",
    "    new_train_label_seg=[]\n",
    "    enough_sample=False\n",
    "    while len(new_train_word)<int(len(training_label)*0.9):\n",
    "        if enough_sample:\n",
    "            break\n",
    "        for i in range(len(ner_idlabel)):\n",
    "            #print('******')\n",
    "            if len(new_train_word)==len(training_label)*0.9:\n",
    "                enough_sample=True\n",
    "                break\n",
    "            \n",
    "            loc_ls=[]\n",
    "            org_ls=[]\n",
    "            per_ls=[]\n",
    "            temp_id_label=np.argmax(training_label[i],axis=1).tolist()\n",
    "            temp_id_label_seg=np.argmax(training_label_seg[i],axis=1).tolist()\n",
    "            temp_id_sent=training_word[i].tolist()\n",
    "            \n",
    "            \n",
    "            temp_label=[]#np.argmax(training_label[i],axis=1).tolist()\n",
    "            temp_label_seg=[]#np.argmax(training_label_seg[i],axis=1).tolist()\n",
    "            temp_label_seg_gold=np.argmax(training_label_seg[i],axis=1).tolist()\n",
    "            temp_sent=[]#training_word[i].tolist()\n",
    "            change_dic={}\n",
    "            loc_temp=[]\n",
    "            org_temp=[]\n",
    "            per_temp=[]\n",
    "            per_begin=-1\n",
    "            per_end=-1\n",
    "            loc_begin=-1\n",
    "            loc_end=-1\n",
    "            org_begin=-1\n",
    "            org_end=-1\n",
    "\n",
    "            reserve=False\n",
    "            for j in range(len(ner_idlabel[i])):\n",
    "                if ner_idlabel[i][j]==0:\n",
    "                    \n",
    "                    if len(loc_temp)>0:\n",
    "                        loc_end=j\n",
    "                        #ner_dict[str(''.join(loc_temp))]=len(ner_dict)\n",
    "                        loc_ls.append(''.join(loc_temp))\n",
    "                        if len(loc_temp)>=2:\n",
    "                            name_rand=unicode(same_sample(''.join(loc_temp),loc_dict))   \n",
    "                            name_tp_list=[]\n",
    "                            for r in name_rand:\n",
    "                                name_tp_list.append(word_dict[str(r)])\n",
    "                            if name_rand!=''.join(loc_temp):\n",
    "                                reserve=True    \n",
    "                            change_dic[loc_begin]=[loc_end,name_tp_list,[0]+[1]*(len(name_tp_list)-1),ner_dict[str(name_rand)].tolist()]\n",
    "                        loc_temp=[]\n",
    "                    if len(org_temp)>0:\n",
    "                        org_end=j\n",
    "                        #ner_dict[str(''.join(org_temp))]=len(ner_dict)\n",
    "                        org_ls.append(''.join(org_temp))\n",
    "                        org_temp=[]\n",
    "                    if len(per_temp)>0:\n",
    "                        per_end=j\n",
    "                        #ner_dict[str(''.join(per_temp))]=len(ner_dict)\n",
    "                        per_ls.append(''.join(per_temp))\n",
    "                        per_temp=[]\n",
    "                    loc_temp.append(numbict[int(training_word[i][j])])\n",
    "                    loc_begin=j\n",
    "                    \n",
    "                if ner_idlabel[i][j]==1  and len(loc_temp)>0:\n",
    "                    loc_temp.append(numbict[int(training_word[i][j])])\n",
    "                    \n",
    "                if ner_idlabel[i][j]==6 and len(loc_temp)>0:\n",
    "                    loc_end=j\n",
    "                    #ner_dict[str(''.join(loc_temp))]=len(ner_dict)\n",
    "                    loc_ls.append(''.join(loc_temp))\n",
    "                    #print(len(loc_temp))\n",
    "                    if len(loc_temp)>=2:\n",
    "                        name_rand=unicode(same_sample(''.join(loc_temp),loc_dict))   \n",
    "                        name_tp_list=[]\n",
    "                        for r in name_rand:\n",
    "                            name_tp_list.append(word_dict[str(r)])\n",
    "                        if name_rand!=''.join(loc_temp):\n",
    "                            reserve=True   \n",
    "                        change_dic[loc_begin]=[loc_end,name_tp_list,[0]+[1]*(len(name_tp_list)-1),ner_dict[str(name_rand)].tolist()]\n",
    "                    loc_temp=[]\n",
    "                    \n",
    "                if ner_idlabel[i][j]==2:\n",
    "                    if len(loc_temp)>0:\n",
    "                        loc_end=j\n",
    "                        #ner_dict[str(''.join(loc_temp))]=len(ner_dict)\n",
    "                        loc_ls.append(''.join(loc_temp))\n",
    "                        loc_temp=[]\n",
    "                    if len(org_temp)>0:\n",
    "                        org_end=j\n",
    "                        #ner_dict[str(''.join(org_temp))]=len(ner_dict)\n",
    "                        org_ls.append(''.join(org_temp))\n",
    "                        if len(org_temp)>=2:\n",
    "                            name_rand=unicode(same_sample(''.join(org_temp),org_dict))   \n",
    "                            name_tp_list=[]\n",
    "                            for r in name_rand:\n",
    "                                name_tp_list.append(word_dict[str(r)])\n",
    "                            if name_rand!=''.join(org_temp):\n",
    "                                reserve=True   \n",
    "                            change_dic[org_begin]=[org_end,name_tp_list,[2]+[3]*(len(name_tp_list)-1),ner_dict[str(name_rand)].tolist()]\n",
    "                        org_temp=[]\n",
    "                    if len(per_temp)>0:\n",
    "                        per_end=j\n",
    "                        #ner_dict[str(''.join(per_temp))]=len(ner_dict)\n",
    "                        per_ls.append(''.join(per_temp))\n",
    "                        per_temp=[]\n",
    "                    org_temp.append(numbict[int(training_word[i][j])])\n",
    "                    org_begin=j\n",
    "                if ner_idlabel[i][j]==3  and len(org_temp)>0:\n",
    "                    org_temp.append(numbict[int(training_word[i][j])])\n",
    "                if ner_idlabel[i][j]==6 and len(org_temp)>0:\n",
    "                    org_end=j\n",
    "                    #ner_dict[''.join(org_temp)]=len(ner_dict)\n",
    "                    org_ls.append(''.join(org_temp))\n",
    "                    if len(org_temp)>=2:\n",
    "                        name_rand=unicode(same_sample(''.join(org_temp),org_dict))   \n",
    "                        name_tp_list=[]\n",
    "                        for r in name_rand:\n",
    "                            name_tp_list.append(word_dict[str(r)])\n",
    "                        if name_rand!=''.join(org_temp):\n",
    "                            reserve=True     \n",
    "                        change_dic[org_begin]=[org_end,name_tp_list,[2]+[3]*(len(name_tp_list)-1),ner_dict[str(name_rand)].tolist()]\n",
    "                    org_temp=[]\n",
    "            \n",
    "                    \n",
    "                if ner_idlabel[i][j]==4:\n",
    "                    if len(loc_temp)>0:\n",
    "                        loc_end=j\n",
    "                        #ner_dict[str(''.join(loc_temp))]=len(ner_dict)\n",
    "                        loc_ls.append(''.join(loc_temp))\n",
    "                        loc_temp=[]\n",
    "                    if len(org_temp)>0:\n",
    "                        org_end=j\n",
    "                        #ner_dict[str(''.join(org_temp))]=len(ner_dict)\n",
    "                        org_ls.append(''.join(org_temp))\n",
    "                        org_temp=[]\n",
    "                    if len(per_temp)>0:\n",
    "                        per_end=j\n",
    "                        #ner_dict[str(''.join(per_temp))]=len(ner_dict)\n",
    "                        per_ls.append(''.join(per_temp))\n",
    "                        if len(per_temp)>=2:\n",
    "                            name_rand=unicode(same_sample(''.join(per_temp),per_dict,True))   \n",
    "                            name_tp_list=[]\n",
    "                            for r in name_rand:\n",
    "                                name_tp_list.append(word_dict[str(r)])\n",
    "                            if name_rand!=''.join(per_temp):\n",
    "                                reserve=True     \n",
    "                            change_dic[per_begin]=[per_end,name_tp_list,[4]+[5]*(len(name_tp_list)-1),ner_dict[str(name_rand)].tolist()]\n",
    "                        per_temp=[]\n",
    "                    per_temp.append(numbict[int(training_word[i][j])])\n",
    "                    per_begin=j\n",
    "                if ner_idlabel[i][j]==5  and len(per_temp)>0:\n",
    "                    per_temp.append(numbict[int(training_word[i][j])])\n",
    "                if ner_idlabel[i][j]==6 and len(per_temp)>0:\n",
    "                    #ner_dict[''.join(per_temp)]=len(ner_dict)\n",
    "                    per_ls.append(''.join(per_temp))\n",
    "                    per_end=j\n",
    "                    if len(per_temp)>=2:\n",
    "                        name_rand=unicode(same_sample(''.join(per_temp),per_dict,True))   \n",
    "                        name_tp_list=[]\n",
    "                        for r in name_rand:\n",
    "                            name_tp_list.append(word_dict[str(r)])\n",
    "                        if name_rand!=''.join(per_temp):\n",
    "                            reserve=True     \n",
    "                        change_dic[per_begin]=[per_end,name_tp_list,[4]+[5]*(len(name_tp_list)-1),ner_dict[str(name_rand)].tolist()]\n",
    "                    per_temp=[]\n",
    "                #print(per_begin,per_end,loc_begin,loc_end,org_begin,org_end)\n",
    "            sparse_temp_label=[]\n",
    "            aug_id=0\n",
    "            while aug_id < len(ner_idlabel[i]):\n",
    "                if aug_id in change_dic.keys():\n",
    "                    temp_sent+=change_dic[aug_id][1]\n",
    "                    temp_label+=change_dic[aug_id][2]\n",
    "                    temp_label_seg+=change_dic[aug_id][3]\n",
    "                    aug_id=change_dic[aug_id][0]-1\n",
    "                else:\n",
    "                    temp_sent.append(temp_id_sent[aug_id])\n",
    "                    temp_label.append(temp_id_label[aug_id])\n",
    "                    temp_label_seg.append(temp_id_label_seg[aug_id])\n",
    "                aug_id+=1\n",
    "            for gg in temp_label:\n",
    "                tpsp=[0,0,0,0,0,0,0]\n",
    "                tpsp[gg]=1\n",
    "                sparse_temp_label.append(tpsp)\n",
    "                \n",
    "            sparse_temp_label_seg=[]\n",
    "            for gg in temp_label_seg:\n",
    "                tpsp=[0,0,0,0]\n",
    "                tpsp[gg]=1\n",
    "                sparse_temp_label_seg.append(tpsp)\n",
    "            #print(len(temp_label_seg))\n",
    "            if len(temp_sent)<=120 and reserve:\n",
    "                if not len(temp_sent)==len(temp_label)==len(temp_label_seg):\n",
    "                    print(len(temp_sent),len(temp_label),len(temp_label_seg))\n",
    "                    assert False\n",
    "                \n",
    "\n",
    "                print(''.join([numbict[a] for a in temp_sent]))\n",
    "                print(temp_label)\n",
    "                print(''.join([numbict[a] for a in training_word[i]]))\n",
    "                print(ner_idlabel[i])\n",
    "                new_train_word.append(temp_sent) \n",
    "                new_train_label.append(sparse_temp_label)\n",
    "                new_train_label_seg.append(sparse_temp_label_seg)\n",
    "                \n",
    "    training_word=[]\n",
    "    training_label=[]\n",
    "    training_label_seg=[]\n",
    "    test_word=[]\n",
    "    test_label=[]\n",
    "    \n",
    "    \n",
    "    maxlen=120\n",
    "    exceed=0\n",
    "    for i in texts:\n",
    "        tp=[]\n",
    "        for j in i:\n",
    "            tp.append(word_dict[str(j)])\n",
    "        tp+=(maxlen-len(tp))*[0]\n",
    "        training_word.append(tp)\n",
    "    \n",
    "    for i in test_texts:\n",
    "        tp=[]\n",
    "        for j in i:\n",
    "            tp.append(word_dict[str(j)])\n",
    "        tp+=(maxlen-len(tp))*[0]\n",
    "        test_word.append(tp)\n",
    "        \n",
    "    for i in label_gold:\n",
    "        training_label.append(i+[[0,0,0,0,0,0,1]]*(maxlen-len(i)))\n",
    "        \n",
    "    for i in test_label_gold:\n",
    "        test_label.append(i+[[0,0,0,0,0,0,1]]*(maxlen-len(i)))\n",
    "    \n",
    "    for i in seg_labels:\n",
    "        training_label_seg.append(i+[[0,0,0,1]]*(maxlen-len(i)))\n",
    "        \n",
    "    maxlen=120\n",
    "    newer_train_word=[]\n",
    "    newer_train_label=[]\n",
    "    newer_train_label_seg=[]\n",
    "    for i in new_train_word:\n",
    "        newer_train_word.append(i+(maxlen-len(i))*[0])\n",
    "        \n",
    "    for i in new_train_label:\n",
    "        newer_train_label.append(i+[[0,0,0,0,0,0,1]]*(maxlen-len(i)))\n",
    "    \n",
    "    for i in new_train_label_seg:\n",
    "        newer_train_label_seg.append(i+[[0,0,0,1]]*(maxlen-len(i)))\n",
    "        \n",
    "    valid_word=  np.array(training_word,dtype='float32')   [random_index][int(sample_number*0.9):]\n",
    "    valid_label= np.array(training_label,dtype='float32')  [random_index][int(sample_number*0.9):]\n",
    "    valid_label_seg= np.array(training_label_seg,dtype='float32')[random_index][int(sample_number*0.9):]\n",
    "    test_word=      np.array(test_word,dtype='float32')\n",
    "    test_label=     np.array(test_label,dtype='float32') \n",
    "    task=[('uselstmsoft',True,False,'none'), ('uselstm',False,False,'none'),\n",
    "          ('uselstm',True,False,'none'),('uselstm',True,False,'peng'),('uselstm',True,True,'none'),\n",
    "          ('usecnnlstmsoft',True,False,'none'),('usecnnlstm',False,False,'none'),('usecnnlstm',True,False,'none'),\n",
    "          ('usecnnlstm',True,False,'joint'),('usecnnlstm',True,True,'none'),('usecnnlstm',False,True,'joint'),('usecnnlstm',True,True,'joint')]\n",
    "\n",
    "    for phase in range(4,5):   \n",
    "        loc_result=[]\n",
    "        org_result=[]\n",
    "        per_result=[]\n",
    "        total_result=[]\n",
    "        oov_result=[]\n",
    "        archi=task[phase][0]\n",
    "        hasemb=task[phase][1]\n",
    "        pseudo=task[phase][2]\n",
    "        joint=task[phase][3]\n",
    "        pseudostr=' '\n",
    "        if not pseudo:       \n",
    "            training_word=  np.concatenate([np.array(training_word,dtype='float32')   [random_index][:int(sample_number*0.9)]])\n",
    "            training_label= np.concatenate([np.array(training_label,dtype='float32')  [random_index][:int(sample_number*0.9)]])\n",
    "            training_label_seg= np.concatenate([np.array(training_label_seg,dtype='float32')[random_index][:int(sample_number*0.9)]])\n",
    "        if pseudo: \n",
    "            pseudostr='+pseudo'\n",
    "            training_word=  np.concatenate([np.array(training_word,dtype='float32')   [random_index][:int(sample_number*0.9)],np.array(newer_train_word)])\n",
    "            training_label= np.concatenate([np.array(training_label,dtype='float32')  [random_index][:int(sample_number*0.9)],np.array(newer_train_label)])\n",
    "            training_label_seg= np.concatenate([np.array(training_label_seg,dtype='float32')[random_index][:int(sample_number*0.9)],np.array(newer_train_label_seg)])\n",
    "\n",
    "        sequenceLength =120\n",
    "        fmin=0\n",
    "        batch_size=64\n",
    "        x_train = Input(batch_shape=(None,sequenceLength),dtype='float32')\n",
    "        \n",
    "        emb_drop=0.2\n",
    "        global_drop=0.2\n",
    "        hid1=200\n",
    "        model0 = Sequential()\n",
    "        embstr=' '\n",
    "        if not hasemb:\n",
    "            embstr='w/o emb'\n",
    "            model0.add(Embedding(len(word_dict), 200,input_length=sequenceLength,mask_zero=False))\n",
    "        else:\n",
    "            model0.add(Embedding(len(word_dict), 200,weights=[charemb],input_length=sequenceLength,mask_zero=False))\n",
    "    \n",
    "        model0.add(Dropout(emb_drop))\n",
    "        \n",
    "        \n",
    "        \n",
    "        e0=model0(x_train)\n",
    "        if archi=='usecnn':\n",
    "            conv2 = Convolution1D(nb_filter=100, filter_length=2, border_mode='same', activation='relu')(e0)\n",
    "            conv3 = Convolution1D(nb_filter=100, filter_length=3, border_mode='same', activation='relu')(e0)\n",
    "            conv4 = Convolution1D(nb_filter=100, filter_length=4, border_mode='same', activation='relu')(e0)\n",
    "            conv5 = Convolution1D(nb_filter=100, filter_length=5, border_mode='same', activation='relu')(e0)\n",
    "            merge = concatenate([conv2, conv3, conv4, conv5])\n",
    "            merge = Dropout(0.2)(merge)\n",
    "            crf = CRF(7,sparse_target=False)\n",
    "            cr=crf(merge)\n",
    "            \n",
    "            \n",
    "        if archi=='uselstm':\n",
    "            blstm1 = Bidirectional(LSTM(200, return_sequences=True), merge_mode='concat')(e0)\n",
    "            blstm1 = Dropout(0.2)(blstm1)\n",
    "            crf = CRF(7,sparse_target=False)\n",
    "            cr=crf(blstm1)\n",
    "            \n",
    "        if archi=='usecnnlstm':\n",
    "            conv2 = Convolution1D(nb_filter=100, filter_length=2, border_mode='same', activation='relu')(e0)\n",
    "            conv3 = Convolution1D(nb_filter=100, filter_length=3, border_mode='same', activation='relu')(e0)\n",
    "            conv4 = Convolution1D(nb_filter=100, filter_length=4, border_mode='same', activation='relu')(e0)\n",
    "            conv5 = Convolution1D(nb_filter=100, filter_length=5, border_mode='same', activation='relu')(e0)\n",
    "            merge = concatenate([conv2, conv3, conv4, conv5])\n",
    "            merge = Dropout(0.2)(merge)\n",
    "    \n",
    "            blstm1 = Bidirectional(LSTM(200, return_sequences=True), merge_mode='concat')(merge)\n",
    "            blstm1 = Dropout(0.2)(blstm1)\n",
    "            crf = CRF(7,sparse_target=False)\n",
    "            cr=crf(blstm1)\n",
    "        if joint=='joint':\n",
    "            crf2 = CRF(4,sparse_target=False)\n",
    "            cr2=crf2(merge)\n",
    "            model=Model([x_train],[cr,cr2])\n",
    "            model.compile('rmsprop', loss=[crf.loss_function,crf2.loss_function], metrics=[crf.accuracy,crf2.accuracy])\n",
    "            model.fit([training_word],[training_label,training_label_seg],\n",
    "                          batch_size=batch_size, nb_epoch=30,shuffle=True,validation_data=([valid_word],[valid_label,valid_label_seg]),callbacks=[EarlyStopping(monitor='val_viterbi_acc', patience=5)])\n",
    "            classes=model.predict([test_word])[0]\n",
    "    \n",
    "        if joint=='none':\n",
    "    \n",
    "            model=Model([x_train],[cr])\n",
    "            model.compile('rmsprop', loss=[crf.loss_function], metrics=[crf.accuracy])\n",
    "            model.fit([training_word],[training_label],\n",
    "                          batch_size=batch_size, nb_epoch=30,shuffle=True,validation_data=([valid_word],[valid_label]),callbacks=[EarlyStopping(monitor='val_viterbi_acc', patience=5)])\n",
    "            classes=model.predict([test_word])\n",
    "            \n",
    "        if joint=='peng':\n",
    "            crf2 = CRF(4,sparse_target=False)\n",
    "            cr2=crf2(blstm1)\n",
    "            model=Model([x_train],[cr,cr2])\n",
    "            model.compile('rmsprop', loss=[crf.loss_function,crf2.loss_function], metrics=[crf.accuracy,crf2.accuracy])\n",
    "            model.fit([training_word],[training_label,training_label_seg],\n",
    "                          batch_size=batch_size, nb_epoch=30,shuffle=True,validation_data=([valid_word],[valid_label,valid_label_seg]),callbacks=[EarlyStopping(monitor='val_viterbi_acc', patience=5)])\n",
    "            classes=model.predict([test_word])[0]\n",
    "    \n",
    "        part=len(test_label)\n",
    "        loc_true_num=0.\n",
    "        org_true_num=0.\n",
    "        per_true_num=0.\n",
    "        loc_pred_num=0.\n",
    "        org_pred_num=0.\n",
    "        per_pred_num=0.\n",
    "        loc_matc_num=0.\n",
    "        org_matc_num=0.\n",
    "        per_matc_num=0.\n",
    "        oov_num=0.\n",
    "        oov_recall_num=0.\n",
    "        \n",
    "        loc_oov_num=0.\n",
    "        org_oov_num=0.\n",
    "        per_oov_num=0.\n",
    "        loc_oov_recall_num=0.\n",
    "        org_oov_recall_num=0.\n",
    "        per_oov_recall_num=0.\n",
    "        pred=np.argmax(classes[:part],axis=2)\n",
    "        true=np.argmax(test_label[:part],axis=2)\n",
    "        for i in range(part):\n",
    "            loc_pred=[]\n",
    "            org_pred=[]\n",
    "            per_pred=[]\n",
    "            loc_temp=[]\n",
    "            org_temp=[]\n",
    "            per_temp=[]\n",
    "            for j in range(len(test_word[i])):\n",
    "                if pred[i][j]==0:\n",
    "                    if len(loc_temp)>0:\n",
    "                        loc_pred.append(str(j)+'_'+''.join(loc_temp))\n",
    "                        loc_temp=[]\n",
    "                    if len(org_temp)>0:\n",
    "                        org_pred.append(str(j)+'_'+''.join(org_temp))\n",
    "                        org_temp=[]    \n",
    "                    if len(per_temp)>0:\n",
    "                        per_pred.append(str(j)+'_'+''.join(per_temp))\n",
    "                        per_temp=[]\n",
    "                    loc_temp.append(numbict[int(test_word[i][j])])\n",
    "                if pred[i][j]==1  and len(loc_temp)>0:\n",
    "                    loc_temp.append(numbict[int(test_word[i][j])])\n",
    "                if pred[i][j]==6 and len(loc_temp)>0:\n",
    "                    loc_pred.append(str(j)+'_'+''.join(loc_temp))\n",
    "                    loc_temp=[]\n",
    "                    \n",
    "                if pred[i][j]==2:\n",
    "                    if len(loc_temp)>0:\n",
    "                        loc_pred.append(str(j)+'_'+''.join(loc_temp))\n",
    "                        loc_temp=[]\n",
    "                    if len(org_temp)>0:\n",
    "                        org_pred.append(str(j)+'_'+''.join(org_temp))\n",
    "                        org_temp=[]    \n",
    "                    if len(per_temp)>0:\n",
    "                        per_pred.append(str(j)+'_'+''.join(per_temp))\n",
    "                        per_temp=[]\n",
    "                    org_temp.append(numbict[int(test_word[i][j])])\n",
    "                if pred[i][j]==3  and len(org_temp)>0:\n",
    "                    org_temp.append(numbict[int(test_word[i][j])])\n",
    "                if pred[i][j]==6 and len(org_temp)>0:\n",
    "                    org_pred.append(str(j)+'_'+''.join(org_temp))\n",
    "                    org_temp=[]\n",
    "        \n",
    "                    \n",
    "                if pred[i][j]==4:\n",
    "                    if len(loc_temp)>0:\n",
    "                        loc_pred.append(str(j)+'_'+''.join(loc_temp))\n",
    "                        loc_temp=[]\n",
    "                    if len(org_temp)>0:\n",
    "                        org_pred.append(str(j)+'_'+''.join(org_temp))\n",
    "                        org_temp=[]    \n",
    "                    if len(per_temp)>0:\n",
    "                        per_pred.append(str(j)+'_'+''.join(per_temp))\n",
    "                        per_temp=[]\n",
    "                    per_temp.append(numbict[int(test_word[i][j])])\n",
    "                if pred[i][j]==5  and len(per_temp)>0:\n",
    "                    per_temp.append(numbict[int(test_word[i][j])])\n",
    "                if pred[i][j]==6 and len(per_temp)>0:\n",
    "                    per_pred.append(str(j)+'_'+''.join(per_temp))\n",
    "                    per_temp=[]\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            loc_true=[]\n",
    "            org_true=[]\n",
    "            per_true=[]\n",
    "            loc_temp=[]\n",
    "            org_temp=[]\n",
    "            per_temp=[]\n",
    "            for j in range(len(test_word[i])):\n",
    "                if true[i][j]==0:\n",
    "                    if len(loc_temp)>0:\n",
    "                        loc_true.append(str(j)+'_'+''.join(loc_temp))\n",
    "                        loc_temp=[]\n",
    "                    if len(org_temp)>0:\n",
    "                        org_true.append(str(j)+'_'+''.join(org_temp))\n",
    "                        org_temp=[]\n",
    "                    if len(per_temp)>0:\n",
    "                        per_true.append(str(j)+'_'+''.join(per_temp))\n",
    "                        per_temp=[]\n",
    "                    loc_temp.append(numbict[int(test_word[i][j])])\n",
    "                if true[i][j]==1  and len(loc_temp)>0:\n",
    "                    loc_temp.append(numbict[int(test_word[i][j])])\n",
    "                if true[i][j]==6 and len(loc_temp)>0:\n",
    "                    loc_true.append(str(j)+'_'+''.join(loc_temp))\n",
    "                    loc_temp=[]\n",
    "                    \n",
    "                if true[i][j]==2:\n",
    "                    if len(loc_temp)>0:\n",
    "                        loc_true.append(str(j)+'_'+''.join(loc_temp))\n",
    "                        loc_temp=[]\n",
    "                    if len(org_temp)>0:\n",
    "                        org_true.append(str(j)+'_'+''.join(org_temp))\n",
    "                        org_temp=[]\n",
    "                    if len(per_temp)>0:\n",
    "                        per_true.append(str(j)+'_'+''.join(per_temp))\n",
    "                        per_temp=[]\n",
    "                    org_temp.append(numbict[int(test_word[i][j])])\n",
    "                if true[i][j]==3  and len(org_temp)>0:\n",
    "                    org_temp.append(numbict[int(test_word[i][j])])\n",
    "                if true[i][j]==6 and len(org_temp)>0:\n",
    "                    org_true.append(str(j)+'_'+''.join(org_temp))\n",
    "                    org_temp=[]\n",
    "                 \n",
    "                    \n",
    "                if true[i][j]==4:\n",
    "                    if len(loc_temp)>0:\n",
    "                        loc_true.append(str(j)+'_'+''.join(loc_temp))\n",
    "                        loc_temp=[]\n",
    "                    if len(org_temp)>0:\n",
    "                        org_true.append(str(j)+'_'+''.join(org_temp))\n",
    "                        org_temp=[]\n",
    "                    if len(per_temp)>0:\n",
    "                        per_true.append(str(j)+'_'+''.join(per_temp))\n",
    "                        per_temp=[]\n",
    "                    per_temp.append(numbict[int(test_word[i][j])])\n",
    "                if true[i][j]==5  and len(per_temp)>0:\n",
    "                    per_temp.append(numbict[int(test_word[i][j])])\n",
    "                if true[i][j]==6 and len(per_temp)>0:\n",
    "                    per_true.append(str(j)+'_'+''.join(per_temp))\n",
    "                    per_temp=[]\n",
    "        \n",
    "                    \n",
    "            loc_true_num+=len(loc_true)\n",
    "            org_true_num+=len(org_true)\n",
    "            per_true_num+=len(per_true)\n",
    "            \n",
    "            loc_pred_num+=len(loc_pred)\n",
    "            org_pred_num+=len(org_pred)\n",
    "            per_pred_num+=len(per_pred)\n",
    "        \n",
    "            for k in loc_true:\n",
    "                if not ner_dict.has_key(str(k.split('_')[1])):\n",
    "                    oov_num+=1\n",
    "                    loc_oov_num+=1\n",
    "                if k in loc_pred:\n",
    "                    loc_matc_num+=1\n",
    "                    if not ner_dict.has_key(str(k.split('_')[1])):\n",
    "                        oov_recall_num+=1\n",
    "                        loc_oov_recall_num+=1\n",
    "                    \n",
    "            for k in org_true:\n",
    "                if not ner_dict.has_key(str(k.split('_')[1])):\n",
    "                    oov_num+=1\n",
    "                    org_oov_num+=1\n",
    "                if k in org_pred:\n",
    "                    org_matc_num+=1 \n",
    "                    if not ner_dict.has_key(str(k.split('_')[1])):\n",
    "                        oov_recall_num+=1\n",
    "                        org_oov_recall_num+=1\n",
    "                    \n",
    "            for k in per_true:\n",
    "                if not ner_dict.has_key(str(k.split('_')[1])):\n",
    "                    oov_num+=1\n",
    "                    per_oov_num+=1\n",
    "                if k in per_pred:\n",
    "                    per_matc_num+=1\n",
    "                    if not ner_dict.has_key(str(k.split('_')[1])):\n",
    "                        oov_recall_num+=1\n",
    "                        per_oov_recall_num+=1\n",
    "        \n",
    "        locp=loc_matc_num/loc_pred_num\n",
    "        locr=loc_matc_num/loc_true_num\n",
    "        locf=2*locp*locr/(locp+locr)\n",
    "        \n",
    "        orgp=org_matc_num/org_pred_num\n",
    "        orgr=org_matc_num/org_true_num\n",
    "        orgf=2*orgp*orgr/(orgp+orgr)\n",
    "        \n",
    "        perp=per_matc_num/per_pred_num\n",
    "        perr=per_matc_num/per_true_num\n",
    "        perf=2*perp*perr/(perp+perr)\n",
    "        all_p=(loc_matc_num+org_matc_num+per_matc_num)/(loc_pred_num+org_pred_num+per_pred_num)\n",
    "        all_r=(loc_matc_num+org_matc_num+per_matc_num)/(loc_true_num+org_true_num+per_true_num)\n",
    "        all_f=2*all_p*all_r/(all_p+all_r)\n",
    "        \n",
    "        print(locp,locr,locf)\n",
    "        print(orgp,orgr,orgf)\n",
    "        print(perp,perr,perf)\n",
    "        print(all_p,all_r,all_f)\n",
    "        print(oov_recall_num/oov_num)\n",
    "        loc_result.append([locp,locr,locf])\n",
    "        org_result.append([orgp,orgr,orgf])\n",
    "        per_result.append([perp,perr,perf])\n",
    "        total_result.append([all_p,all_r,all_f,oov_recall_num/oov_num])\n",
    "        oov_result.append([loc_oov_recall_num/loc_oov_num,org_oov_recall_num/org_oov_num,per_oov_recall_num/per_oov_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
